{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b05a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "import collections\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "from collections import Counter\n",
    "print(tf_build_info.build_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/models/data/truss_rebalanced.csv\", quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"material_cost_per_truss\", \"build_cost_per_truss\", \"setup_cost_per_run\"], axis=1)\n",
    "y = data[[\"material_cost_per_truss\", \"build_cost_per_truss\", \"setup_cost_per_run\"]]\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)\n",
    "print(X.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b71624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# BUILD COST CLASSIFICATION MODEL DATA SETUP\n",
    "# ==================================================\n",
    "\n",
    "# --- 1. Prepare target and encode labels ---\n",
    "y_build = y[[\"build_cost_per_truss\"]].copy()  # Single target column\n",
    "enc_build = LabelEncoder()\n",
    "y_build_labels = enc_build.fit_transform(y_build.values.ravel())  # Convert to class IDs (0..N-1)\n",
    "\n",
    "# --- Merge rare classes with nearest neighbor ---\n",
    "counts = Counter(y_build_labels)\n",
    "class_info = pd.DataFrame({\n",
    "    \"class_id\": np.arange(len(enc_build.classes_)),\n",
    "    \"build_cost\": enc_build.classes_.astype(float),\n",
    "    \"count\": [counts.get(i, 0) for i in range(len(enc_build.classes_))]\n",
    "})\n",
    "\n",
    "threshold = 100  # minimum samples per class\n",
    "y_merged = y_build_labels.copy()\n",
    "\n",
    "# Sort by build cost for easier neighbor lookup\n",
    "class_info_sorted = class_info.sort_values(\"build_cost\").reset_index(drop=True)\n",
    "\n",
    "for _, row in class_info_sorted.iterrows():\n",
    "    cls = row[\"class_id\"]\n",
    "    count = row[\"count\"]\n",
    "\n",
    "    if count < threshold:\n",
    "        cost = row[\"build_cost\"]\n",
    "\n",
    "        # Find valid classes (â‰¥ threshold)\n",
    "        valid_classes = class_info_sorted[class_info_sorted[\"count\"] >= threshold]\n",
    "\n",
    "        # Find the nearest valid class by cost difference\n",
    "        nearest_idx = (valid_classes[\"build_cost\"] - cost).abs().idxmin()\n",
    "        nearest_class = valid_classes.loc[nearest_idx, \"class_id\"]\n",
    "        nearest_cost = valid_classes.loc[nearest_idx, \"build_cost\"]\n",
    "\n",
    "        # Merge (relabel) all occurrences\n",
    "        y_merged[y_merged == cls] = nearest_class\n",
    "\n",
    "        print(f\"Class {int(cls):2d} (${cost:.2f}, {count} samples) â†’ merged into \"\n",
    "              f\"Class {int(nearest_class):2d} (${nearest_cost:.2f})\")\n",
    "\n",
    "new_counts = Counter(y_merged)\n",
    "\n",
    "df_counts = pd.DataFrame({\n",
    "    \"Class_ID\": list(new_counts.keys()),\n",
    "    \"Build_Cost_$\": [float(enc_build.classes_[int(cls)]) for cls in new_counts.keys()],\n",
    "    \"Samples\": list(new_counts.values())\n",
    "}).sort_values(\"Build_Cost_$\").reset_index(drop=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)  # show all rows\n",
    "print(df_counts)\n",
    "    \n",
    "y_build_labels = y_merged\n",
    "\n",
    "num_classes_build = len(enc_build.classes_)\n",
    "build_cost_levels = enc_build.classes_.astype(float)  # Actual $ values per class\n",
    "\n",
    "# --- 2. Split (stratified) ---\n",
    "x_train_build, x_test_build, y_train_build_lbl, y_test_build_lbl = train_test_split(\n",
    "    X, y_build_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# --- 3. Scale features ---\n",
    "x_scaler_build = StandardScaler()\n",
    "x_train_build_scaled = x_scaler_build.fit_transform(x_train_build)\n",
    "x_test_build_scaled  = x_scaler_build.transform(x_test_build)\n",
    "\n",
    "# --- 4. Compute class weights (optional but helpful) ---\n",
    "# All possible classes (model expects 0 to num_classes-1)\n",
    "all_classes = np.arange(num_classes_build)\n",
    "\n",
    "# Classes actually present in training data\n",
    "present_classes = np.unique(y_train_build_lbl)\n",
    "\n",
    "# Compute weights only for present classes\n",
    "weights_arr = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=present_classes,\n",
    "    y=y_train_build_lbl\n",
    ")\n",
    "\n",
    "# Map weights to present classes\n",
    "present_weights = dict(zip(present_classes, weights_arr))\n",
    "\n",
    "# Build full class_weight dict for model\n",
    "class_weights_build = {\n",
    "    int(cls): float(present_weights.get(cls, 1.0))  # default to 1.0 if class not in training\n",
    "    for cls in all_classes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24840b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Build classification model ---\n",
    "model_build_cls = models.Sequential([\n",
    "    layers.Input(shape=(x_train_build_scaled.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(num_classes_build, activation='softmax')  # Classification head\n",
    "])\n",
    "\n",
    "model_build_cls.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3_acc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 6. Train ---\n",
    "print(\"Training Build Cost CLASSIFIER...\")\n",
    "\n",
    "reduce_lr_build = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5\n",
    ")\n",
    "early_stop_build = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "with tf.device('/GPU:0'):\n",
    "    history_build_cls = model_build_cls.fit(\n",
    "        x_train_build_scaled, y_train_build_lbl,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr_build, early_stop_build], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5124fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: C:/models/build_cost_price_model\\assets\n",
      "âœ… Build cost model now outputs actual $ value per sample.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tensor of actual price values (replace with your enc_build.classes_ array)\n",
    "build_class_prices = tf.constant(enc_build.classes_.astype(np.float32))\n",
    "\n",
    "# Input for post-processing wrapper\n",
    "inp = tf.keras.Input(shape=(x_train_build_scaled.shape[1],))\n",
    "probs = model_build_cls(inp)\n",
    "class_idx = tf.argmax(probs, axis=1, output_type=tf.int32)\n",
    "pred_price = tf.gather(build_class_prices, class_idx)\n",
    "\n",
    "model_build_price = tf.keras.Model(inputs=inp, outputs=pred_price, name=\"build_cost_price_model\")\n",
    "model_build_price.save(\"C:/models/build_cost_price_model\")\n",
    "print(\"âœ… Build cost model now outputs actual $ value per sample.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# EVALUATE BUILD COST CLASSIFIER (CLEAN REPORT)\n",
    "# ==================================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error, mean_squared_error\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Make predictions on test set ---\n",
    "probs_test_build = model_build_cls.predict(x_test_build_scaled)\n",
    "pred_build_lbl = probs_test_build.argmax(axis=1)\n",
    "pred_build_argmax = build_cost_levels[pred_build_lbl]\n",
    "\n",
    "# Optional: compute \"expected value\" for smoother continuous predictions\n",
    "pred_build_ev = probs_test_build @ build_cost_levels\n",
    "y_true_build = build_cost_levels[y_test_build_lbl]\n",
    "\n",
    "# ==================================================\n",
    "# VISUALIZE TRAINING HISTORY (BUILD COST MODEL)\n",
    "# ==================================================\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history_build_cls.history['loss'], label='Training Loss')\n",
    "plt.plot(history_build_cls.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Build Cost Model Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==================================================\n",
    "# VISUAL COMPARISONS\n",
    "# ==================================================\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(y_true_build, pred_build_argmax, alpha=0.7, edgecolor='k')\n",
    "plt.plot([y_true_build.min(), y_true_build.max()],\n",
    "         [y_true_build.min(), y_true_build.max()],\n",
    "         'r--', lw=2)\n",
    "plt.title(\"Actual vs Predicted Build Cost per Truss (Argmax Prediction)\")\n",
    "plt.xlabel(\"Actual Build Cost ($)\")\n",
    "plt.ylabel(\"Predicted Build Cost ($)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(y_true_build, pred_build_ev, alpha=0.7, color='orange', edgecolor='k')\n",
    "plt.plot([y_true_build.min(), y_true_build.max()],\n",
    "         [y_true_build.min(), y_true_build.max()],\n",
    "         'r--', lw=2)\n",
    "plt.title(\"Actual vs Predicted Build Cost per Truss (Expected Value Prediction)\")\n",
    "plt.xlabel(\"Actual Build Cost ($)\")\n",
    "plt.ylabel(\"Predicted Build Cost ($)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ==================================================\n",
    "# COMPUTE METRICS\n",
    "# ==================================================\n",
    "report_lines = []\n",
    "report_lines.append(\"=== BUILD COST CLASSIFIER EVALUATION REPORT ===\\n\")\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "cm_build = confusion_matrix(y_test_build_lbl, pred_build_lbl)\n",
    "report_lines.append(\"ðŸ“Š Confusion Matrix (numeric class IDs):\\n\")\n",
    "report_lines.append(pd.DataFrame(cm_build).to_string(index=False))\n",
    "report_lines.append(\"\\n\")\n",
    "\n",
    "# --- Classification Report ---\n",
    "report_lines.append(\"ðŸ“‹ Classification Report (Precision, Recall, F1-Score):\\n\")\n",
    "class_report = classification_report(y_test_build_lbl, pred_build_lbl, digits=3)\n",
    "report_lines.append(class_report)\n",
    "report_lines.append(\"\\n\")\n",
    "\n",
    "# --- Overall Accuracy ---\n",
    "correct_preds = np.trace(cm_build)\n",
    "total_preds = np.sum(cm_build)\n",
    "accuracy = correct_preds / total_preds\n",
    "report_lines.append(\"âœ… OVERALL ACCURACY:\\n\")\n",
    "report_lines.append(f\"The model correctly classified {accuracy*100:.2f}% of build-cost samples.\\n\\n\")\n",
    "\n",
    "# --- Per-Class Accuracy ---\n",
    "class_totals = cm_build.sum(axis=1)\n",
    "correct_per_class = np.diag(cm_build)\n",
    "per_class_accuracy = correct_per_class / class_totals\n",
    "per_class_table = pd.DataFrame({\n",
    "    \"Class ID\": np.arange(len(per_class_accuracy)),\n",
    "    \"Accuracy (%)\": (per_class_accuracy * 100).round(2)\n",
    "})\n",
    "report_lines.append(\"ðŸŽ¯ PER-CLASS ACCURACY (% of correctly predicted samples per cost bin):\\n\")\n",
    "report_lines.append(per_class_table.to_string(index=False))\n",
    "report_lines.append(\"\\n\")\n",
    "\n",
    "# --- Identify worst misclassification ---\n",
    "errors = pred_build_lbl != y_test_build_lbl\n",
    "wrong_idxs = np.where(errors)[0]\n",
    "if len(wrong_idxs) > 0:\n",
    "    confidences = probs_test_build[np.arange(len(probs_test_build)), y_test_build_lbl]\n",
    "    worst_idx = wrong_idxs[np.argmin(confidences[wrong_idxs])]\n",
    "\n",
    "    worst_true = y_test_build_lbl[worst_idx]\n",
    "    worst_pred = pred_build_lbl[worst_idx]\n",
    "    worst_prob = probs_test_build[worst_idx][pred_build_lbl[worst_idx]]\n",
    "    true_conf = confidences[worst_idx]\n",
    "\n",
    "    report_lines.append(\"ðŸ˜¬ WORST CLASSIFICATION (lowest confidence on true class):\\n\")\n",
    "    report_lines.append(f\"True Class: {worst_true} | Predicted Class: {worst_pred}\\n\")\n",
    "    report_lines.append(f\"Predicted Probability: {worst_prob:.2f} | Confidence on True Class: {true_conf:.2f}\\n\\n\")\n",
    "else:\n",
    "    report_lines.append(\"âœ… No misclassifications detected!\\n\\n\")\n",
    "\n",
    "# ==================================================\n",
    "# DOLLAR-LEVEL ERROR METRICS\n",
    "# ==================================================\n",
    "y_true_dollars = build_cost_levels[y_test_build_lbl]\n",
    "y_pred_dollars = build_cost_levels[pred_build_lbl]\n",
    "\n",
    "mae = mean_absolute_error(y_true_dollars, y_pred_dollars)\n",
    "rmse = np.sqrt(mean_squared_error(y_true_dollars, y_pred_dollars))\n",
    "dollar_errors = np.abs(y_true_dollars - y_pred_dollars)\n",
    "max_error = dollar_errors.max()\n",
    "large_miss_idx = np.where(dollar_errors > 20)[0]\n",
    "\n",
    "report_lines.append(\"ðŸ’° DOLLAR-LEVEL PERFORMANCE METRICS:\\n\")\n",
    "report_lines.append(f\"- Mean Absolute Error (MAE): ${mae:.2f}\\n\")\n",
    "report_lines.append(\"  â†’ Average difference between predicted and actual build cost.\\n\")\n",
    "report_lines.append(f\"- Root Mean Square Error (RMSE): ${rmse:.2f}\\n\")\n",
    "report_lines.append(\"  â†’ Typical magnitude of prediction error, weighted toward larger misses.\\n\")\n",
    "report_lines.append(f\"- Maximum Absolute Error: ${max_error:.2f}\\n\")\n",
    "report_lines.append(\"  â†’ The single largest dollar deviation observed.\\n\")\n",
    "report_lines.append(f\"- Samples with >$20 error: {len(large_miss_idx)} out of {len(dollar_errors)} total.\\n\")\n",
    "report_lines.append(\"  â†’ Extreme outliers beyond a $20 difference.\\n\\n\")\n",
    "\n",
    "# ==================================================\n",
    "# CLASS DISTRIBUTION\n",
    "# ==================================================\n",
    "counts = Counter(y_train_build_lbl)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(counts.keys(), counts.values(), color=\"slateblue\")\n",
    "plt.title(\"Training Class Distribution (Build Cost)\")\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"Samples per Class\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Optional: summary printout\n",
    "print(\"\\n=== Summary Metrics ===\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Mean Absolute Error (MAE): ${mae:.2f}\")\n",
    "print(f\"Root Mean Square Error (RMSE): ${rmse:.2f}\")\n",
    "print(f\"Maximum Absolute Error: ${max_error:.2f}\")\n",
    "print(f\"Samples >$20 error: {len(large_miss_idx)} / {len(dollar_errors)}\")\n",
    "\n",
    "# ==================================================\n",
    "# PRINT FEATURES OF LARGE-ERROR PREDICTIONS\n",
    "# ==================================================\n",
    "error_threshold = 10\n",
    "large_error_idx = np.where(dollar_errors > error_threshold)[0]\n",
    "\n",
    "if len(large_error_idx) > 0:\n",
    "    print(f\"\\n=== ðŸ” Predictions with Error > ${error_threshold} ===\")\n",
    "    \n",
    "    # Convert scaled test set to DataFrame for readability\n",
    "    feature_names = getattr(x_test_build_scaled, 'columns', None)\n",
    "    if feature_names is None:\n",
    "        # if it's a NumPy array, make generic feature names\n",
    "        feature_names = [f\"feature_{i}\" for i in range(x_test_build_scaled.shape[1])]\n",
    "    \n",
    "    df_large_errors = pd.DataFrame(\n",
    "        x_test_build_scaled[large_error_idx],\n",
    "        columns=feature_names\n",
    "    )\n",
    "    df_large_errors[\"Actual ($)\"] = y_true_dollars[large_error_idx]\n",
    "    df_large_errors[\"Predicted ($)\"] = y_pred_dollars[large_error_idx]\n",
    "    df_large_errors[\"Absolute Error ($)\"] = dollar_errors[large_error_idx]\n",
    "\n",
    "    # Sort by absolute error descending\n",
    "    df_large_errors = df_large_errors.sort_values(\"Absolute Error ($)\", ascending=False)\n",
    "\n",
    "    # Print top few rows and save to CSV for review\n",
    "    print(df_large_errors.head(20).to_string(index=False))\n",
    "    df_large_errors.to_csv(f\"large_error_predictions_{timestamp}.csv\", index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved all {len(df_large_errors)} large-error samples to 'large_error_predictions_{timestamp}.csv'\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No samples found with error greater than ${error_threshold}.\")\n",
    "    \n",
    "    \n",
    "# First, attach class IDs (based on nearest build_cost_levels)\n",
    "def nearest_class_id(values, class_levels):\n",
    "    return [np.argmin(np.abs(class_levels - v)) for v in values]\n",
    "\n",
    "df_large_errors[\"Actual Class\"] = nearest_class_id(df_large_errors[\"Actual ($)\"], build_cost_levels)\n",
    "df_large_errors[\"Pred Class\"]   = nearest_class_id(df_large_errors[\"Predicted ($)\"], build_cost_levels)\n",
    "\n",
    "# Optional: readable labels\n",
    "df_large_errors[\"Actual Label\"] = [build_cost_levels[i] for i in df_large_errors[\"Actual Class\"]]\n",
    "df_large_errors[\"Pred Label\"]   = [build_cost_levels[i] for i in df_large_errors[\"Pred Class\"]]\n",
    "\n",
    "# Pick the span-related feature (looks like feature_6 drives most outliers)\n",
    "span_feature = \"feature_6\"\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sc = plt.scatter(\n",
    "    df_large_errors[\"Actual Class\"],\n",
    "    df_large_errors[\"Pred Class\"],\n",
    "    c=df_large_errors[span_feature],\n",
    "    cmap=\"viridis\",\n",
    "    s=80,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"k\"\n",
    ")\n",
    "plt.colorbar(sc, label=f\"{span_feature} (scaled)\")\n",
    "plt.title(\"High-Error Samples: Actual vs. Predicted Class\")\n",
    "plt.xlabel(\"Actual Class ID\")\n",
    "plt.ylabel(\"Predicted Class ID\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "def export_build_cost_classifier(model, x_scaler, label_encoder, class_weights, export_dir=\"build_cost_classifier\"):\n",
    "    \"\"\"\n",
    "    Exports a trained build cost classification model, scaler, label encoder, and metadata.\n",
    "    \"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # --- Save model ---\n",
    "    model.save(os.path.join(export_dir, \"model_build_classifier.keras\"))\n",
    "\n",
    "    # --- Save preprocessing tools ---\n",
    "    joblib.dump(x_scaler, os.path.join(export_dir, \"x_scaler_build.pkl\"))\n",
    "    joblib.dump(label_encoder, os.path.join(export_dir, \"label_encoder_build.pkl\"))\n",
    "    joblib.dump(class_weights, os.path.join(export_dir, \"class_weights_build.pkl\"))\n",
    "\n",
    "    # --- Metadata ---\n",
    "    metadata = {\n",
    "        \"num_classes\": len(label_encoder.classes_),\n",
    "        \"class_labels\": label_encoder.classes_.astype(float).tolist()\n",
    "    }\n",
    "    joblib.dump(metadata, os.path.join(export_dir, \"metadata_build.pkl\"))\n",
    "\n",
    "    print(f\"âœ… Build Cost Classifier exported to '{export_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_build_cost_classifier(\n",
    "    model_build_cls,\n",
    "    x_scaler_build,\n",
    "    enc_build,\n",
    "    class_weights_build,\n",
    "    export_dir=\"exported_build_classifier\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
